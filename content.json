{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"스타트업에서 효율적으로 분석환경 구축하기","text":"본 자료는 19년 7월 데이터뽀개기에서 발표한 본인의 슬라이드를 기반으로 만들었습니다. 본 자료에 해당하는 작업은 입사 초에 했던 업무로 현재의 분석환경과 많은 기술적 차이가 있습니다. 지금은 이렇지 않아요. 들어가기 전에중소 규모의 스타트업은 항상 개발자가 부족합니다. 때문에 대부분의 개발 리소스는 급하고 필수적인 곳에 쓰이기 마련입니다. 만약 여유가 생긴다고 해도 다른 중요한 개발 부채를 처리하기 바쁘지, 분석환경 개선 같은 당장 진행하지 않아도 운영에 지장이 없는 업무는 우선순위가 밀리기 마련입니다. 그러다가 사내에서 특정 데이터의 중요성이 대두되고 중요한 의사결정을 하기 위해 개선이 필요해진다면 그 때 관련한 리소스를 지원받을 수 있을겁니다. 그렇다면 분석가는 정말 그때까지 기다려야 할까요? 오늘은 개발 리소스가 부족한 중소 규모의 스타트업에서 개발자 없이 간편하게 분석환경을 구축할 수 있는 방법에 관해 이야기 하려합니다. 이미 구조가 잘 갖춰진 곳에서 분석을 하시는 분들이나 개발 리소스가 넉넉한 곳에서 일하시는 분들이라면 이렇게 일하는 곳도 있구나! 정도로 읽어주시면 되겠습니다. 용어 정리본 글을 읽는데 도움이 되는 용어를 짧게 설명 드립니다. 데이터 웨어하우스(Data Warehouse) / 데이터 마트(Data Mart) 사람들은 장을 볼 때 물류창고에 가지 않습니다. 물건이 장 정리된 마트를 가거나 앱을 사용할 것입니다. 데이터도 마찬가지 입니다. 큰 물류창고에서 장을 보기 쉽지 않은 것 처럼 (불가능하겠죠) 데이터 웨어하우스에서 데이터를 분석하는 일은 어려운 일입니다. 물류창고는 마트와 달리 장보는 사람을 배려하지 않기 때문입니다. 데이터 웨어하우스는 데이터가 한 곳에 모이는 물류창고, 데이터 마트는 물건이 잘 정리된 마트 정도로 생각하시면 본 글의 내용을 충분히 이해할 수 있습니다. ETL(Extract, Transform, Load) / ELT(Extract, Load, Transform) ETL은 데이터의 추출, 변환, 적재를 의미합니다. 위에서 설명한 데이터 웨어하우스/마트를 예로 들면, 데이터 마트를 구축하기 위해서 데이터 웨어하우스에 있는 데이터를 1)추출하고 분석하기 편한 방법으로 2)변환한 다음 마트에 3)적재하는 작업을 진행합니다. 이 모든 작업을 통틀어서 ETL이라 합니다. 요즘에는 클라우드 컴퓨팅의 발달로 실질적인 작업이 ELT의 형태를 띄는 경우가 많을 것 같습니다. ELT는 말 그대로 데이터의 1)추출, 2)저장 그리고 3)변환을 의미합니다. 옛날에 비해 저장 비용이나 데이터 처리 성능이 훨씬 좋아졌기에 일단 저장하고 변환한다는 의미에서 ELT라고도 부릅니다. 저도 대부분의 작업은 ELT 방식으로 처리합니다만 개발자들과 커뮤니케이션 할 때는 항상 ETL이라 부르네요. 문제 인식데이터 분석가로 합류한지 한 달이 채 되지 않았을 무렵. 당시의 분석환경이 지속가능한 구조가 아니라는 생각이 들었습니다. 단순한 추출이나 집계에도 구조적인 문제로 로컬 작업이 많았고, 로컬 작업이 많다보니 데이터 관련 업무는 시스템이 아닌 개인에게 의지를 많이 했습니다. 문제를 해결하기 위해 당시 문제점을 아래와 같이 정리하였습니다. 1. 데이터 웨어하우스의 부재 여러가지 종류의 DB(Postgresql, Oracle, Mongo, Bigquery, Mysql)와 DB로 존재하지 않는 데이터(Appsflyer, CS솔루션, 호텔, 보험, 렌트카) 등이 혼재된 분석환경 데이터를 통합해서 보기 어려움→ 데이터를 BI툴이 아닌 로컬에서 확인하는 이유 2. 데이터 마트의 부재 복잡한 DB 구조 및 원천 데이터의 오류로 간단한 추출에도 쿼리 안에 많은 전처리 로직이 들어감(distinct, group by, row_number) 단순/반복 작업에 시간을 많이 씀→ 원천 데이터의 오류를 해결하는 서브쿼리나 매번 들어가는 조건을 매번 붙여넣기 3. DB성능 2번의 이슈로 DB가 계속 죽음 쿼리 최적화 및 DB운영과 관련된 지속적인 리소스 필요→ feat. show process list &amp; sig kill 4. 데이터 관련 개발 리소스의 부재 위 이슈를 해결할 개발 리소스의 부재 대부분의 개발 리소스는 1,3번에 쓰인다 그렇다면1) 운영 리소스가 적게 들어가는 DB를 선정하고2) 데이터를 한 곳에 모을 데이터 웨어하우스를 만든 다음3) 이후 전처리한 데이터 마트를 만들어 단순 반복작업을 줄이면 되겠다.4) 개발자 없이 문제 해결1. DB 선정제일 중요한 작업입니다. DB를 선택할 때는 비용, 확장성, 운영 리소스, 성능 등 다양한 기준이 있습니다. 당시만 해도 회사에는 현재의 DP(Data Platform)팀이 없었기에 운영 리소스를 최소한으로 가져가야 했고 그런 기준으로 선택한 것은 구글의 빅쿼리 입니다. 일단 빅쿼리는 쉽습니다. 누구든 로그인 후 (결제카드 등록하고) SQL만 수행하면 됩니다. 또 인스톨이나 별도의 설정 등 유지보수도 필요 없고 투자 없이 수천 개의 CPU와 컴퓨팅 자원이 사용가능합니다. (구글 조대협님 빅쿼리 소개글 참고: 링크) 이전에 AWS의 Redshift, Athena등을 사용해보았지만 쿼리 튜닝이나 다른 구글 제품(GA, Firebase, Spreadsheet)과의 호환성을 생각할 때 다른 클라우드 서비스보다 빅쿼리를 훨씬 더 유용하게 사용할 수 있다고 판단했습니다. (추후에는 GA-빅쿼리의 연동 등 다양한 아티클로 찾아뵙겠습니다) 2. 데이터 웨어하우스 구축이제 데이터를 빅쿼리로 모으기만하면 데이터 웨어하우스가 완성되겠네요. 방법에는 여러가지가 있습니다. 아파치 Airflow 같은 도구로 직접 코드를 짜서 배치를 돌릴 수도 있고 상용화된 클라우드 ETL 서비스를 사용할 수도 있습니다. 각각의 장단점이 있지만 당시의 저는 운영을 최소한으로 가져가고 싶었기에 클라우드 ETL 서비스를 선택하였습니다. (이게 나중에 독이될줄..) 1) 클라우드 ETL 서비스 생각보다 많은 클라우드 ETL 서비스가 있습니다. 각 서비스마다 연동가능한 소스(DB, API), 적재 지원 DB, 싱크 주기, 비용 등 천차만별이니 각자 본인의 상황에 알맞는 업체를 찾으면 됩니다. 다양한 서비스의 리서치 이후 3개의 업체를 컨택하였고 견적서를 받았습니다. 그리고 (애증의) Stitch를 선택하게 되었네요. 스티치는 정말 간단합니다. 원하는 DB나 서비스를 선택하고 정보만 입력하면 데이터를 잘 쌓아줍니다. 비용은 데이터 row수에 비례합니다. 데이터의 양에 따라 가격차이가 좀 있겠지만 리서치한 업체 중에는 가장 저렴했습니다. 2) 그 외 ETL 하지만 스티치는 모든 데이터 소스를 커버하지 못했습니다. 대표적으로 Oracle은 엔터프라이즈 요금제를 사용하는 회사만 사용할 수 있었고 당시 견적서의 금액은 이럴거면 개발자를 고용하지! 라는 말이 나올 정도로 비쌌는데요. 그래서 나머지는 Python, Bigquery Data Transfer Service, 테이블-구글시트 연동 등 다양한 방법으로 구현했습니다. 당시 사용한 파이썬 라이브러리로는 BigQuery Client Library, Pandas GBQ, Gspread_pandas, Gcsfs 정도가 있겠네요. 다양한 라이브러리를 사용한 만큼 다양한 오류를 겪었습니다. 이 글에서 다루기에는 너무 딥한 주제라 기회가 된다면 다른 글에서 소개하겠습니다. 3. 데이터 마트 구축어렵게 데이터를 한데 모았습니다. 이제 마트만 만들면 되겠네요. 마트는 사실 최근까지 고도화를 하고 있는 만큼 정답이 있는 영역은 아닌 것 같습니다. 아래는 효율적인 마트를 만드는 방법이라기 보다 이런게 필요해서 이렇게 만들었다 정도로 읽어주시면 되겠습니다. 당시 데이터 웨어하우스에서 통합예약을 조회하려면 말도 안되게 많은 조인을 했어야합니다. (초기에는 이 작업을 로컬에서…) 통합예약을 만드는 쿼리를 만들고 배치를 돌려 마트에 저장하고 분석하는 사람들은 그렇게 생성된 마트만 본다면 좀 더 효율적으로 분석을 할 수 있겠죠? 현재 마트에는 다양한 테이블이 존재하는데 데이터를 보는 분들의 피드백을 받아 지금까지 계속 고도화를 하고 있습니다. 그럼 마트를 만드는 배치는 어떻게 돌릴까요? 다양한 방법이 있습니다. 위에서 잠깐 언급한 에어플로우를 사용할 수도 있고, 별도의 인스턴스를 만들어서 크론잡을 만들 수도 있습니다. 이런 방법 외 제일 쉬운 방법은 구글에서 제공하는 기능을 이용하는 것입니다. 바로 쿼리예약(Scheduled Query) 이라는 기능입니다. 쿼리만 짤 수 있으면 누구든 배치잡을 만들 수 있고 DML(Data Manipulation Language)도 지원하기에 생각보다 다양한 일을 할 수 있습니다. 다만 당시에는 쿼리로 집계하기 어려운 영역도 일부 있어 별도의 인스턴스에서 파이썬과 구글의 쿼리예약을 섞어 썼습니다. 요약 요약하면 각종 서비스를 통해 데이터를 빅쿼리에 잘 모으고. 쿼리예약과 파이썬을 통해 데이터를 예쁘게?.. 만든 다음 BI툴에 연결해서 데이터를 보는 구조라고 생각하면 됩니다. 사실 완벽한 하지는 않습니다. 버그에 취약하고 각 task간 의존성이 없기에 초기 데이터 수집 단계에서 뻑이나면 다시 전체를 돌리거나(혹은 다시 배치를 기다리거나..) 노가다를 해야하는 부분이 일부 있었습니다. 다만 반드시 완벽한 데이터 웨어하우스나 마트가 있어야 의미있는 분석을 할 수 있는 것은 아닙니다. 이런 방식으로라도 데이터를 보는 것이 안보는 것 보다는 훨씬 유의미한 결과를 가져왔다고 생각합니다. 성과사실 분석환경 개선의 진정한 의미는 분석가 뿐만 아니라 쿼리에 익숙치 않은 다양한 실무팀에서도 필요한 데이터를 추출할 수 있는 환경을 만들었다에 의의가 있다고 생각합니다. (데이터가 흐르는 조직!) 그리고 분석가는 기존에 전처리나 단순 추출에 쓰이던 시간을 많이 아낄 수 있었고, 그 시간을 회사의 성장을 위한 좋은 질문을 고민하는데 더 많은 시간을 쓴게 가장 큰 성과가 아닐까 생각이 되네요. 한계너무 좋은 말만 쓰면 안되겠죠. 이런 데이터 파이프라인은 디버깅에 취약합니다. 뭐가 안되어도 왜 안되는지 알 수 없고 안다고 해도 서비스 벤더가 버그를 수정해줄 때까지 기다리는 수밖에 없습니다. 종종 데이터의 정합성이 깨지는 오류가 발견이 되었는데 제가 할 수 있는 일은 삭제하고 다시 쌓는 것 밖에 었습니다. 문의를 넣어봐도 지우고 다시 연동해봐라! 정도의 답변이 최선이었습니다. (국적을 가리지 않는 껏켰) 나름 다행인 것은 정말 작은 차이라 초반에는 발견도 하지 못했습니다. 장기적으로 ETL서비스의 의존도를 줄이고 인하우스로 우리 서비스에 알맞는 데이터 파이프라인을 설계해야 더 안정적이고 견고한 분석환경을 만들어 갈 수 있습니다. 발표할 당시에만 해도 DP팀이 열심히 구현 중이었는데 글로 정리하는 지금 시점에는 이미 완성이 되었네요. 뿌듯합니다.","link":"/2020/05/06/data_environment_in_startup/"}],"tags":[{"name":"data","slug":"data","link":"/tags/data/"},{"name":"data_env","slug":"data-env","link":"/tags/data-env/"}],"categories":[{"name":"data","slug":"data","link":"/categories/data/"},{"name":"data_env","slug":"data/data-env","link":"/categories/data/data-env/"}]}